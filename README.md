# Memory-Based-ChatBot
# ğŸ§  Memory-Based Chatbot with Long-Term Context (FAISS + Streamlit)

This project implements a **memory-augmented conversational chatbot** that can remember **user identity, past conversations, and private contextual information across sessions**.

Unlike a general chatbot that sends only the current user input to an LLM, this system **retrieves the top-5 most relevant historical memories from a FAISS vector database** and injects them into the prompt, enabling **context-aware and personalized responses**.



## ğŸš€ Key Features

âœ” Long-term memory using **FAISS vector database**  
âœ” Short-term conversation memory with **timestamps**  
âœ” Persistent **user identity memory**  
âœ” **Top-5 relevant memory retrieval** per query  
âœ” **RAG-based prompt generation**  
âœ” Suggestions & related memories display  
âœ” Custom memory storage  
âœ” Streamlit chat-style UI  
âœ” Secure API key handling using `.env`



## ğŸ¤– General Chatbot vs Memory-Based Chatbot

| Feature | General Chatbot | Memory-Based Chatbot (This Project) |
|------|----------------|--------------------------------------|
| Input to LLM | Only current message | Current message + retrieved memories |
| Memory | Session-based | Long-term + short-term |
| Context after refresh | âŒ Lost | âœ… Preserved |
| Personalization | âŒ No | âœ… Yes |
| Vector DB | âŒ Not used | âœ… FAISS |
| RAG | âŒ No | âœ… Yes |
| Suggestions | âŒ No | âœ… Yes |
| Timestamps | âŒ No | âœ… Yes |



## ğŸ§  How the Chatbot Works (Architecture Flow)

### 1ï¸âƒ£ User Input
The user enters a message using the Streamlit chat interface.



### 2ï¸âƒ£ Embedding Generation
The user input is converted into a numerical vector using the **EURI Embeddings API**.



### 3ï¸âƒ£ Memory Retrieval (FAISS)
- The embedding is searched against stored vectors in **FAISS**
- The **top 5 most relevant memories** are retrieved
- These memories may include:
  - Previous conversations
  - User identity
  - Custom user-defined memories
  - Past private context



### 4ï¸âƒ£ Prompt Generation (RAG)
A **Retrieval-Augmented Generation (RAG)** prompt is constructed using:
- User identity
- Retrieved relevant memories
- Current user input

This allows the LLM to generate **context-aware responses**.



### 5ï¸âƒ£ Response Generation
The prompt is sent to the **EURI Chat Completion API**, which generates the assistant response.



### 6ï¸âƒ£ Memory Update
- The interaction is stored back into FAISS
- Conversation history is saved with **date & time**
- Memory persists across sessions


## ğŸ” Types of Memory Used

### ğŸ”¹ Short-Term Memory
- Current conversation
- Stored with timestamps
- Can be cleared by the user

### ğŸ”¹ Long-Term Memory
- Stored in FAISS
- Includes identity, interactions, custom memory
- **Not removed when conversation is cleared**
- Enables long-term personalization

---

## â±ï¸ Timestamped Conversations

Each user and assistant message is stored with:
- Date
- Time

This improves traceability and conversation analysis.

---

## ğŸ’¡ Suggestions & Related Memories

For every query:
- The top-5 similar memories are retrieved
- Displayed as **Suggestions & Related Memories**
- Helps explain how the response was generated

---

## ğŸ—ï¸ Tech Stack

- **Python 3.10+**
- **Streamlit** â€“ Frontend UI
- **FAISS (CPU)** â€“ Vector similarity search
- **NumPy** â€“ Vector operations
- **Requests** â€“ API communication
- **python-dotenv** â€“ Environment variables
- **EURI AI APIs** â€“ Embeddings & Chat Completion



